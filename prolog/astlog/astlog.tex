\secrel{ASTLOG: Язык для анализа синтаксических деревьев}\secdown
\cp{\url{http://www.cs.nyu.edu/~lharris/papers/crew.pdf}}
\copyright\ Roger F. Crew \email{rfc@microsoft.com}\\
Microsoft Research
Microsoft Corporation
Redmond, WA 98052

\secly{Abstract}

We desired a facility for locating/analyzing syntactic artifacts in abstract
syntax trees of \ci/\cpp\ programs, similar to the facility \prog{grep} or
\prog{awk} provides for locating artifacts at the lexical level. \prolog, with
its implicit pattern-matching and backtracking capabilities, is a natural choice
for such an application. We have developed a \prolog\ variant that avoids the
overhead of translating the source syntactic structures into the form of a
\prolog\ database; this is crucial to obtaining acceptable performance on large
programs. An interpreter for this language has been implemented and used find
various kinds of syntactic bugs and other questionable constructs in real
programs like \prog{Microsoft SQL server} (450Klines) and \prog{Microsoft Word}
(2Mlines) in time comparable to the runtime of the actual compiler.

The model in which terms are matched against an implicit current object, rather
than simply proven against a database of facts, leads to a distinct ``inside-out
functional'' programming style that is quite unlike typical \prolog, but one
that is, in fact, well-suited to the examination of trees. Also, various second-order
\prolog\ set-predicates may be implemented via manipulation of the current
object, thus retaining an important feature without entailing that the database
be dynamically extensible as the usual implementation does.

\secrel{Introduction}\secdown

Tools like \prog{grep} and \prog{awk} are useful for finding and analyzing
lexical artifacts; e.g., a one-line command locates all occurences of a
particular string. Unfortunately, many simple facts about programs are less
accessible at the character/token level, such as the locations of assignments to
a particular \cpp\ class member. In general, reliably extracting such syntactic
constructs requires writing a parser or some fragment thereof. And after writing
one's twenty-seventh parser fragment, one might begin to yearn for a more
general tool capable of operating at the syntax-tree level.

Even given a compiler front-end that exposes the abstract syntax tree (AST)
representation for a given program, there remains the question of what exactly
to do with it. To be sure, supplying a \ci\ programmer with a sufficiently
complete interface to this representation generally solves any problem one might
care to pose about it. One may just as easily say that all problems at the
lexical level may be solved via proper use of the UNIX standard IO library
\verb|<stdio.h>|, a true, but utterly trivial and unsatisfying statement. The
question is rather that of building a simpler, more useful and flexible
interface: one that is less error-prone, more concise than writing in \ci, and
more directly suited to the task of exploring ASTs. We first consider a couple
of prior approaches.

\secrel{The \prog{awk} Approach}

One of the more popular approaches is to extend the \prog{awk} \cite{AKW86}
paradigm. An \prog{awk} script is a list of pairs, each being a
regular-expression with an accompanying statement in a C-like imperative
language. For each line in the input file, we consider each pair of the script
in turn; if the regular-expression matches the line, the corresponding statement
is executed.

Extending this to the AST domain is straightforward, though with numerous
variations. One defines a regular-expression-like language in which to express
tree patterns and an \prog{awk}-like imperative language for statements. The
tree nodes of the input program are traversed in some order (e.g., preorder),
and for each node the various pairs of the script are considered as before.

We have two objections to this approach, the first having to do with the
hardwired framework that usually implicit. In some cases (e. g., \prog{TAWK}
\cite{GA96}), the traversal order for the AST nodes is essentially fixed; using
a different order would be analogous to attempting to use plain \prog{awk} to
scan the lines of a text file in reverse order. In $A*$ \cite{LR95}, while the
user may define a general traversal order, only one traversal method may be
defined/active at any given time, making difficult any structure comparisons
between subtrees or other applications that require multiple concurrent
traversals. Since the imperative language is quite general in both cases, little
is deffinitively impossible, however for some applications one may be little
better off than when programming in straight \ci.

The second objection has to do with the kinds of pattern-abstraction available.
Inevitably there exist simply-described patterns that are a poor fit to a
regular-expression-like syntax. This tends to happen when said simple
descriptions are in terms of the idioms of a particular programming language;
most of the various tree-\prog{awk} pattern languages tend to be designed with
the intent of being language independent.

Suppose one wishes to find all consecutive occurrences of one statement
immediately preceding another, e. g., places where a given system call
\verb|syscall();| is followed immediately by an \verb|assert();| \note{on the
theory that testing of outcomes of system calls should be done in production
code rather than just debugging code}. A tree-regular-expression pattern of the
form

\begin{verbatim}
<syscall() pattern>; <assert() pattern>
\end{verbatim}

\noindent
(where \verb|;| is the regular-expression sequence operator) finds all instances
of the two calls occurring consecutively within a single block, but it misses
instances like

\begin{verbatim}
syscall();
{
    assert();
    ...
}
\end{verbatim}

and

\begin{verbatim}
if (...) {
    syscall();
}
else {
    ...
}
assert();
\end{verbatim}

While the tree-\prog{awk} languages allow one to write patterns to match each of
these cases, without a pattern-abstraction facility, we may be back at square
one when it comes time to look for some \emph{different} pair of consecutive
function calls. We prefer to write a single consecutive-statement pattern
constructor \emph{once} and then be able to use it for a variety of cases where
we need to find pairs of consecutive statements satisfying certain criteria,
invoking it as

\begin{verbatim}
follow_stmt(<syscall() pattern>, <assert() pattern>)
\end{verbatim}
for the above problem, or, if we instead want to be fiding all of the places
where a \ci\ switch-case falls through, as
\begin{verbatim}
follow_stmt(not(<unconditional-jump pattern>),
                <case-labeled stmt pattern>)
\end{verbatim}

One solution, used by \prog{TAWK}, is to use \prog{cpp}, the C preprocessor, to
preprocess the script, allowing for pattern-abstractions to be expressed as
\verb|#define| macros whose invocations are then expanded as needed. This is
unsatisfactory in a number of ways, whether one wants to consider the problem of
recursively-defined patterns, macros with large bodies that result in a
corresponding blow-up in the size of the script, or the difficulty of tracing
script errors that resulted from complex macro-expansions.

Another way out is to fall back on the procedural abstraction available in the
imperative language that the patterns invoke. One essentially uses a degenerate
pattern that always matches and then allows the imperative code to test whether
the given node is in fact the desired match, defining functions to test for
particular patterns. Once again, it seems we are back to programming in straight
C and not deriving as much benefit from having a pattern language available as
we could be.

In general, the philosophical underpinning of the \prog{awk} approach is that
the designer has already determined the kinds of searches the user will want to
do; the effort is put towards making those particular searches run efficiently.
There is also an assumption that the underlying imperative language for the
actions has all the abstraction facilities one will ever need, so that if the
pattern language is lacking in various ways, this is not deemed a serious
problem. While this is not an unreasonable approach, we have less confidence of
having identified all of the reasonable search possibilities, and thus would
prefer instead to make the pattern language more flexible and extensible, being
willing to sacrifice some efficiency to do so.

\secrel{The Logic Programming Approach}

Another common approach is to run an inference engine over a database of program
syntactic structures \cite{BCD88, BGV90, CMR92}. \prolog\ \cite{SS86} is a
convenient language for this sort of application. Backtracking and a form of
pattern matching are built in, the abstraction mechanisms to build up complex
predicates exist at a fundamental level, andfinally, \prolog\ allows for a more
declarative programming style.

The problems with using \prolog\ are two-fold. First there is the issue of
efficiency. Second, we must represent the AST for our source program in the
\prolog\ database. Large programs ($10^5..10^6$ lines) will result in
correspondingly large \prolog\ databases, most likely with a significant
performance penalty.

We finesse the second problem by not attempting to import the source program's
AST at all, instead opting to modify the interpretation of the predicates and
queries of \prolog\ so as to be applicable to external objects rather than just
facts provable in the existing database. Removing reasons that require the
database to grow beyond the initial script creates significant opportunities for
optimization. This, however, requires removing primitives like \verb|assert()|
and \verb|retract()| that allow for the dynamic (re)defiition or removal of
predicates, which in turn removes many higher-order logical features that are
defined in terms of them. Fortunately, some of the more essential ones can be
restored at relatively little cost.

\secup

\secrel{Elements of ASTLOG}\secdown

Section \ref{crewsyntax} gives the complete syntax for our language, ASTLOG. The
ASTLOG interpreter reads a script of user-defined predicate operator definitions
and then runs one or more queries.

As in \prolog, the definition of a user-defined predicate operator is composed
of one or more \term{clauses}. A compound term \verb|opname(term,...)| appearing
at top level in a clause body is interpreted as a predicate, whether
\term{opname} be primitive or user-defined. In the latter case, the script is
searched for a defining clause whose head terms successfully unify with the
respective operand terms of the given compound term, variables are bound
accordingly, and the terms of the clause body are likewise interpreted. The
clause \emph{succeeds} (i. e., is found to be true) if all of its body terms
succeed. Whenever a clause head fails to unify, or a clause body term
\emph{fails} (i. e., is found to be false), or any primitive term fails by the
rules of evaluation of that primitive, we backtrack to the last point where
there was a choice (e. g., of clauses to try for a given compound term) and
continue.

A \term{query} is a clause whose head terms are all variables. Ultimately,
whenever all terms of a query body succeed, the bindings of any variables listed
in the query head (\term{qhead}) are reported. Otherwise, we report failure.
Thus far, this is all exactly like \prolog.

\secrel{Complete Syntax of ASTLOG}\label{crewsyntax} 

\begin{tabular}{l l l}
script & ::= named-clause* & script file syntax \\
query & ::= imports? ( varname* ) clause-body ; & query syntax \\
imports & ::= \{ varname+ \} &\\
named-clause & ::= opname anon-clause &\\
anon-clause & ::= ( term* ) clause-body? ; &\\
clause-body & ::= <- term+ &\\
\end{tabular}

\noindent
\begin{tabular}{l l l}
\hline
&Essential Term Syntax&\\
\hline
term & ::= literal & reference to denotable ob ject \\
& ::= varname &\\
& ::= opname ( term* ) & compound term \\
& ::= FN imports? ( anon-clause+ ) & anonymous predicate-operator-valued 
\\&&(``lambda'') term \\
& ::= ' opname arity-spec? & named predicate-operator-valued
\\&&(``function quote'') term \\
& ::= ( term )( term* ) & ``application'' term \\ 
\end{tabular}

\noindent
\begin{tabular}{l l l}
\hline
&Gratuitous Term Syntax&\\
\hline
&::= \# constname & named constant\\&&($\equiv$ corresponding literal number)\\
&::= \verb|[ term* ]| & \verb|[ ]| $\equiv$ nil(), \verb|[term]| $\equiv$
cons(term; nil()), etc\ldots\\
&::= \verb$[ term+ | term ]$ & \verb$[ term1 | term2 ]$ $\equiv$
cons(term1,term2), etc\ldots\\
arity-spec & ::= / integer &\\ 
\end{tabular}

\secrel{Objects}

ASTLOG refers to external objects. Given a \ci/\cpp\ compiler front end that
provides a (\cpp) interface to the syntactic/semantic data structures built
during the parse of a given program, it is simple to graft this onto the core of
ASTLOG so that it may recognize object references corresponding to
\begin{itemize}[nosep]
  \item whole C/C++ programs,
  \item single files,
  \item symbols,
  \item AST nodes (including statements, expressions, and declarations), and
  \item \ci/\cpp\ type descriptions.
\end{itemize}

For the purposes of ASTLOG, an \term{object} is simply some external entity that
is significant for its identity and for the primitive predicates that it may
satisfy. To simplify the language we regard the traditional
constants (integers, floats, and strings) to be references to ``external''
objects as well, though one could just as easily take the converse view in which
the universe of object references is just a (very large) pool of
constants\note{``atoms'' in the usual \prolog\ terminology}.

In any case, object references are terms in ASTLOG. Only references to equal
objects can unify, equality meaning numeric equality for numbers,
same-sequence-of-characters for strings, and identity for all other classes of
objects. Only objects that have denotations (numbers, strings and the unique
\verb|null object*|) can find their way into scripts.

\secrel{The Current Object}



\secup

\secup