\secrel{PARSING}\label{cohen2}

In this section we present parsers belonging to two main classes of parsing
algorithms, namely, bottom-up and top-down. Due to the backtracking capabilities
of Prolog, these parsers can in general handle nondeterministic and
ambiguous languages. An early paper by Griffiths and Petrick [18] describes
various parsing algorithms and their simulation by automata. There the amount
of nondeterminism is roughly specified by a selectivity matrix which guides the
parser in avoiding states leading to backtracking. A similar situation occurs in
the Prolog parsers described here. In compilers, interest is commonly restricted 
to deterministic languages. Backtracking may be prevented by a judicious use of
cuts(!) and/or by introducing assertions in the database that guide the parser in
avoiding dead-ends. 

A word about notation is in order. The grammar conventions are those in [l].
Edinburgh Prolog uses capital letters as variables, and therefore capitals cannot
be used to represent nonterminals unless they are quoted. In this paper, the
terms t( ) and n( ) denote, respectively, terminals and nonterminals. Quoting
may be necessary for specifying certain terminals (e.g., parentheses). For example,
the right-hand side (rhs) of the rule
\[F \rightarrow (E)\]
is described by the list 
\[[WV), de), W)‘)l. \]

Whenever stacks are used, they are also represented by lists whose leftmost
element is the top of the stack. 

This section does not pretend to make an exhaustive treatment of parsers.
We describe bottom-up and top-down parsers for both nondeterministic and
deterministic languages. A nondeterministic shift-reduce and a deterministic
weak-precedence parser are the bottom-up representatives. Their top-down counterparts
are, respectively, a predictive and an LL(1) parser. A recursive descent
version of the latter is also considered. Besides those described herein, we have
programmed and tested Earley’s algorithm [13] and a parser generator that
produces the necessary tables for parsing SLR( 1) grammars [ 11]. 

\secdown

\secrel{Bottom-Up}

A very simple (albeit inefficient) shift-reduce parser can be readily programmed
in Prolog. Its action consists of attempting to reduce whenever possible; otherwise
the window is shifted on to a stack and repeated reductions (followed by shifts)
take place until the main nonterminal appears by itself in the top of the stack.
Note that a reduction may be immediately followed by other reductions. A
reduction corresponds to the recognition of a grammar rule; for instance, the
reduction for the rule E + E + T occurs when E + T lies on the top of the stack.
It is then replaced by an E. This action is expressed by the unit clause
\[redme(Idt), H+), n(e) I Xl, Me) I Xl). \]

Let us consider the classical grammar describing arithmetic expressions: 
\[G1: E+E+T\]
\[E-T\]
\[T-+ T*F\]
\[T+F\]
\[F + (E)\]
\[F + (letter)\]
The appropriate sequence of reduce clauses follows immediately from the above
rules. To decrease the amount of backtracking it is convenient to order these
clauses so that rules with longer right-hand sides are tried before those with 
shorter rhs. We are now ready to present the parser. It has two parameters:
(1) a list representing the string being parsed, and (2) the list representing the
current stack. 
\begin{verbatim}
% try-reduce %
sr-parse(Input, Stack) :- reduce(Stack, NewStack),
% try-shift %
sr-parse(Input, NewStack).
sr-parse([ Window I Rest], Stack) :- sr-parse(Rest, [Window 1 Stack]). 
\end{verbatim} 
Assume that a marker (\$) is to be placed at the end of each input string. The
following acceptance clause accepts a string only when the marker is in the
window and the stack contains just an E.
\begin{verbatim}
% acceptance %
sr-paWL§l, Me)lh 
\end{verbatim}
Consider the input string a*b. We assume that a scanner is available to translate
it into the suitable list, understandable by the parser. Then the query
\begin{verbatim}
?- sr-PamWa), t(*), t(b), $1, [ I).
\end{verbatim}
will succeed. 

The above parser is very inefficient, since it relies heavily on backtracking to
eventually accept or refuse a string. Note that in parsing the string a*b, t(a) is
first shifted and successively reduced to an F, T, and (even) an E; the latter
being a faulty reduction. The parser is, however, capable of undoing these
reductions through backtracking. This inordinate amount of backtracking can
be controlled by a careful selection of the reductions and shifts that eliminate
possible blind-alleys. This is done in our next bottom-up parser, which is the
weak-precedence type [ 1,19].

The basic strategy is to consult a table made of unit clauses like
\[try-redue(Top-of-stack, Window). and try_shift(Top-of-stack, Window).\]
which command a reduction or a shift, depending on the elements lying on the
window and on the top of the stack. The problem of automatically generating
the above clauses from the grammar rules is addressed in Section 5. The weakprecedence
relations for the grammar Gi are represented by the clauses 
\[try_reduce(n(t), \$).\]
\[try_reduce(n(f), \$9.\]
\[. . .\]
\[try-reduce(t(‘)‘), t(+)).\]
\[\&-reduce(t(‘)‘), t(‘)‘)).\]
and
\[try-shift(t(+), t(‘(‘)).\]
\[tryshift(n(e), t(+)).\]
and so on.    

We now transform the previous sr-parser into a wp-parser which takes
advantage of the additional information to avoid backtracking. Using 
Griffiths and Petrick’s terminology [ 18], these unit clauses render the
algorithms selective. 
\begin{verbatim}
% acceptance %
w-~~~~4$1, [de)l).
% try-reduce %
wp-parse([ W 1 Input], [S 1 Stack]) :- try_reduce(S, W),
reduce([S 1 Stack], NewStack),
wp-purse([ W 1 Input], NewStack).
% try shift %
wp-parse([ WI Input], [S 1 Stack]) :- try-shift(S, W),
wp-purse(Zrzput, [W, S 1 Stack]). 
\end{verbatim}
Notice that if a grammar is truly a weak-precedence grammar (i.e., there are no
precedence conflicts and rules have distinct rhs), then backtracking will only
occur when try-reduce fails and try-shift has to be tried. Thus the query
\begin{verbatim}
?- wp-parse(Znput, [ I), print(accept).
\end{verbatim}
will print “accept”, and succeed if the Input string is in the language. If the string
is not in the language, the query will fail. The time complexity is proportional to
the length of Input. Error detection and recovery are discussed in Section 9.

A comment about the efficiency of this version of wp-parse is in order. Since
there will in general be a large number of try-reduce and try-shift rules, the
execution time of the wp-parser could be significantly reduced if a Prolog compiler
could branch directly to a clause having the appropriate constant as its first term
(for example, by constructing a hashing table at compile time). Recent and
planned Prolog optimizing compilers can indeed perform this branching [30].
The reader should also refer to [21] for a discussion of optimizations applicable
to deterministic Prolog programs, which render their efficiency closer to those of
conventional programs. 

Finally, note that it would be straightforward to extend this type of parser to
cover the syntactical analysis of bounded-context grammars, that is, those for
which a decision to reduce or shift is based on an inspection of m elements in
the top of the stack and a look-ahead of n elements in the input string. 

\secrel{Top-Down}

A Prolog implementation of predictive parsers [l] follows readily from the
programs described in the previous section. The grammar G2, below, generates
the same language as G1, but left-recursion has been replaced by right-recursion. 
\[Gz: E -+ TE’\]
\[E’ -+ + TE’\]
\[E’ 3 e\]
\[T +FT’\]
\[T’ --B * FT’\]
\[T’ + E\]
\[F *U-O\]
\[F + (letter)\]
The above rules are placed in the database using the unit clauses
\begin{verbatim}
rule(Non-terminal, Rhs).
\end{verbatim}
Examples are
\begin{verbatim}
ruMn(tprim), [t(*), n(f), nt@hne)l).
rule(n(tprime), [ I). 
\end{verbatim}

The parser predict(Input, Stack) has the same parameters as its predecessors,
namely: (1) the input string and (2) the current stack contents (initially n(e),
where E is the main nonterminal). The parser succeeds if the Input string is in
the language, and fails otherwise. 

The basic action of predict is to replace a nonterminal on the top of the parse
stack by the rhs of the rule defining that nonterminal. If a terminal element lies
on the top of the stack and if it matches the element W in the window, then
parsing proceeds by popping W and considering the next element of the input
string to be in the window. A string is accepted when the stack is empty and the
window contains the marker. In Prolog we have 
\begin{verbatim}
% acceptance.
predict(I$l, 1 I).
% try a possible rule.
predict(lnput, [n(N) 1 Stack]) :-
ruldn(N), Rh),
append(Rhs, Stack, NewStack),
predict&put, NewStack).
% match the terminals.
predict([t(W) 1 Input], [t(W) 1 Stack]) :- predict(lnput, Stack). 
\end{verbatim}
The above parser can handle nondeterministic or even ambiguous grammars, but
may become trapped in an infinite recursion loop if the grammar is left-recursive. 

To improve the efficiency when processing deterministic grammars, one could
again resort to placing additional information in the database. This is the case
for the next parser we consider, which is applicable to LL( 1) grammars, and does
not rely on backtracking. It will become apparent in Section 5 that it is straightforward
to generate tables for LL( 1) grammars [ 1]. These tables have as entries
the contents of the window t(W) and the nonterminal n(N) on the top of the
stack, and they specify the appropriate (unique) replacement by the rhs of the
rule defining N. Entries may be defined by unit clauses of the form 
\begin{verbatim}
entv(t(W), n(N), Rhs).
\end{verbatim}
for all pairs ( W, N) such that N J* W . . . . An LL(1) deterministic parser is
obtained by replacing the middle clause of predict by
\begin{verbatim}
predict[t( W) 1 Input], [n(N) 1 Stack]) :- entry(t(W), n(N), Rhs),
append(Rhs, Stack, NewStack),
predict([t(W) I Input], NewStack).   
\end{verbatim}

By properly selecting one among multiple entries, predict can deterministically
parse languages defined by ambiguous grammars, as is the case of the if then
else construct considered in [l, p. 191]. Moreover, the parser does not rely on
backtracking to accept a string. The complexity of the LL(1) parser is therefore
O(n) where n is the length of the input string.

\secrel{Recursive Descent }

All of the previously described parsers contain a general nucleus which drives
the parsing, the grammar rules being specified by unit clauses in the database.
Parser efficiency can be increased by establishing a direct mapping between
grammar rules and Prolog clauses. This is accomplished as in recursive descent
parsing: each procedure directly corresponds to a given grammar rule. As usual,
left-recursion is not allowed and has to be replaced by right-recursion to avoid
endless loops. 

There are three manners in which these parsers can be implemented in Prolog,
depending on the form of the input string. The first and the least efficient of
these is the one that uses the predicate append. The second uses links to define
the input string that appears as unit clauses in the database. Finally, the third,
which uses difference lists, is the most efficient, as will be seen by estimates of
the various complexities. The implementation of these versions is illustrated
using the grammar G3, generating a’kb”, n I 0. The notation t(T) and n(N) will
no longer be needed to differentiate between terminals and nonterminals, since
the nonterminals will be transformed into Prolog procedures which manipulate
terminal strings. 
\[G3: S+aSb\]
\[S4C\]
Every grammar rule is transformed into a clause whose argument is the list of
terminals derived from the defined nonterminal. Terminals are similarly handled
using unit clauses. We have 
\begin{verbatim}
s(ASB) :- uppend(A, SB, ASB),
~PP~W, B, SB),
a(A),
SW,
MB).
s(C) :- c(C).
ml).
WI).
4cl).   
\end{verbatim}
The appends are used to partition the list ASB as the concatenation of three
sublists A, S, B. Although the only partition for which the parser will succeed is
\[A = a, S = an-lcbnml, B = b,\]
this program will generate at least 2n incorrect partitions. Hence the number of
calls needed to append is at least n2. Note that the appends should precede the
calls of a(A), s(S), b(B). Otherwise, an infinite loop would occur. The above
program can be optimized by symbolic execution: the terms a(A), b(B), and c(C)
can be directly replaced by their unit clause counterparts, yielding 
\begin{verbatim}
s(ASB) :- uppend([a], SB, ASB),
wwMS, PI, SB),
SW.
ml).  
\end{verbatim}

The second approach for programming recursive descent parsers in Prolog is
the use of links. An input string such as [a, a, c, b, b] is represented by the unit
clauses link(i, t, i + l), stating that there is a terminal t located between positions
i and i + 1. In our case the input string aacbb becomes
hk(1, a, 2).
\begin{verbatim}
link(2, a, 3).
link(3, c, 4).
link(4, b, 5).
link(5, b, 6). 
\end{verbatim}
A clause recognizing a nonterminal will now have two parameters denoting the
leftmost and rightmost positions in the input string that will parse into the given
nonterminal. In our particular example we have
\begin{verbatim}
s(X1, X4) :- link(X1, a, X2),
s(X2, X3),
link(X3, b, X4).
s(X1, X2) :- link(X1, c, X2). 
\end{verbatim}
The as will be consumed by the n successive calls of the first two literals. Then,
only the second clause is applicable and the c is consumed. Finally, the unbound
variables X3, X4 are successively bound to the points separating the remaining
bs. The algorithm’s complexity is therefore linear. 

An efficient implementation of recursive descent parsers in Prolog makes use
of difference lists. If a nonterminal A generates a terminal string a! (i.e., A ==a*
a), that string can be represented by the difference of two lists U and V; V is a
sublist of U which has the same tail as U. For example, if U is [a, c, b, b, b] and
V is [b, b] the difference U - V defines the list [a, c, b], which for G3 parses into
an S. Warren [31] points out that the use of difference lists corresponds to having
the general link-like clause:
\begin{verbatim}
link([H 1 2’1, H, T) 
\end{verbatim}
which can be read as “the string position labelled by the list with head H and
tail T is connected by a symbol H to the string position labelled T.” A parser for
G3 using difference lists can be written as follows:
\begin{verbatim}
s(U, V) :- a(U, Vl), s(V1, V2), b(V2, V).
s( w, 2) :- c( w, 2). 
\end{verbatim}
For the terminals a, b, and c we have
\begin{verbatim}
am I VII, vu.
b([b I U21, U2).
c(k I u31, U3). 
\end{verbatim}
Symbolic execution allows us to find the values of U and Vl in the first clause:
\begin{verbatim}
U=[alUl], Vl=Ul 
\end{verbatim}
Similarly,
\begin{verbatim}
V2=[bIU2], V= U2
W=[cIu@], z=u3 
\end{verbatim}
Substituting the values of the above variables, we obtain the optimized program
\begin{verbatim}
s([a 1 Ul], U2) :- s(U1, [b I U2]).
4kl u31, U3). 
\end{verbatim}
(The above program could also have been derived using symbolic execution
by considering the first version of the parser with append and noticing that
if X - Y and Y - Z are difference lists, then append(X - Y, Y - 2, X - 2) is
a fact.) 

Let us follow the execution of the call
\[s(b, 0, c, b, bl, [ I).\]
Notice that Ul becomes [a, c, b, b] and U2 is [ ]. The next calls of S are
\[~([a, c, h bl, PII\]
\[s(k, h bl, P, bl)\] 
This last call matches only the second clause thus indicating a valid string. An
informal English description of the acceptance is as follows: successively remove
each a in the head of the first of the difference lists and add a b to the second
one. A string is accepted when no more as can be removed, the head of the first
list is a c, and the two lists contain the same number of bs. Therefore, for this
particular grammar, G3, the parsing is done in linear time with no backtracking.
The reader might have already surmised that the use of difference lists and of
symbolic executions illustrated in this example could be carried out automatically
from the given grammar rules. Clocksin and Mellish ([6, 1st ed., p. 237-2381]
present a short Prolog program that does the translation. 

\secup

