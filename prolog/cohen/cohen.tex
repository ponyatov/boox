\secrel{Parsing and Compiling Using Prolog}\label{cohen}\secdown
\href{https://drive.google.com/file/d/0B0u4WeMjO894eHpLcTE2bWU0SjQ/view?usp=sharing}{pdf}
\cp{\url{https://pdfs.semanticscholar.org/dd8d/c0deb336d90912a21ba8ec6f6c6fef4b4024.pdf}}

\copyright\ JACQUES COHEN and TIMOTHY J. HICKEY\\
Brandeis University
\bigskip

\subsecly{Abstract}

This paper presents the material needed for exposing the reader to the
advantages of using Prolog as a language for describing succinctly most of the
algorithms needed in prototyping and implementing compilers or producing tools
that facilitate this task. The available published material on the subject
describes one particular approach in implementing compilers using Prolog. It
consists of coupling actions to recursive descent parsers to produce
syntax-trees which are subsequently utilized in guiding the generation of
assembly language code. Although this remains a worthwhile approach, there is a
host of possibilities for Prolog usage in compiler construction. The primary aim
of this paper is to demonstrate the use of Prolog in parsing and compiling. A
second, but equally important, goal of this paper is to show that Prolog is a
labor-saving tool in prototyping and implementing many nonnumerical algorithms
which arise in compiling, and whose description using Prolog is not available in
the literature. The paper discusses the use of unification and nondeterminism in
compiler writing as well as means to bypass these (costly) features when they
are deemed unnecessary. Topics covered include bottom-up and top-down parsers,
syntax-directed translation, grammar properties, parser generation, code
generation, and optimixations. Newly proposed features that are useful in
compiler construction are also discussed. A knowledge of Prolog is assumed.

Categories and Subject Descriptors: D.l.O [Programming Techniques]: General;
D.2.m [Software Engineering]: Miscellaneous--rapid prototyping; D.3.4
[Programming Languages]: Processors; F.4.1. [Mathematical Logic and Formal
Languages]: Mathematical Logic--logic programming 1.2.3 [Artificial
Intelligence]: Deduction and Theorem Proving--logic programming

General Terms: Algorithms, Languages, Theory, Verification

Additional Key Words and Phrases: Code generation, grammar properties,
optimization, parsing

\bigskip
This work was supported by the NSF under grant DCR 8590881.

Authors’ address: Computer Science Department, Ford Hall, Brandeis University, Waltham, MA
02254.

Permission to copy without fee all or part of this material is granted provided that the copies are not
made or distributed for direct commercial advantage, the ACM copyright notice and the title of the
publication and its date appear, and notice is given that copying is by permission of the Association
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific
permission.

0 1987 ACM 0164-0925/87/0400-0125 \$00.75

ACM Transactions on Programming Languages and Systems, Vol. 9, No. 2, April 1967, Pages 125-163. 

\secrel{INTRODUCTION}

The seminal paper by Alain Colmerauer on Metamorphosis Grammars first
appeared in 1975 [9]. That paper spawned most of the developments in compiler
writing using Prolog, a great many of them due to David H. D. Warren. Warren’s
thesis [30], the paper summarizing it [31], and the related work on Definite
Clause Grammars [25] are practically the sole sources of reference on the
subject.\note{A recent book edited by Campbell [3] mostly covers the
implementation of Prolog itself. }

The available published material on the subject describes one particular approach
in implementing compilers using Prolog. It consists of coupling actions 
to recursive descent parsers to produce syntax-trees which are subsequently
utilized in guiding the generation of assembly language code. Although this
remains a worthwhile approach, there is a host of possibilities for Prolog usage
in compiler construction. The primary aim of this paper is to present the material
needed for exposing the reader to the advantages of using Prolog in parsing and
compiling. A second, but equally important, goal of this paper is to show that
Prolog is a labor-saving tool in prototyping and implementing many nonnumerical
algorithms which arise in compiling, and whose description using
Prolog is not available in the literature. Finally, a third goal is to present new
approaches to compiler design which use proposed extensions to Prolog. 

This paper is directed to compiler designers moderately familiar with Prolog,
who wish to explore the advantages and present drawbacks of using this language
for implementing language processors. The advantages of Prolog stem from two
important features of the language. 

(1) The use of unification as a general pattern-matching operation allowing
procedure parameters (logical variables) to be both input and output or to
remain unbound. Unification replaces the conditionals and assignments
which exist in most languages. 

(2) The ability to cope with nondeterministic situations, and therefore allow the
determination of multiple solutions to a given problem. 

From a subjective point of view, the main advantage of Prolog is that the
language has its foundations in logic, and it therefore encourages the user to
describe problems in a logical manner which facilitates the checking for correctness,
enhances program readability, and reduces the debugging effort. It will be
seen that unification and nondeterminism play an important role in compiler
design; however, using their full generality is often costly and unnecessary. These
issues are discussed throughout the paper whenever they become relevant.
Remarks are made in the last section about the efficiency of Prolog-written
compilers and the means to improve their performance. 

The Prolog proficiency assumed in this paper can be acquired by reading the
first few chapters of either Kowalski’s [20] or Clocksin and Mellish’s [6] books.
In particular, the reader should be at ease with elementary list processing and
with the predicate append. The concrete syntax used in this paper is that of
Edinburgh Prolog [6]. It is also assumed that the reader is familiar with compiler
design topics such as parsing, lexical analysis, code generation, optimizations,
and so on. These topics are covered in standard texts [l, 17,29]. 

\secrel{PARSING}

In this section we present parsers belonging to two main classes of parsing
algorithms, namely, bottom-up and top-down. Due to the backtracking capabilities
of Prolog, these parsers can in general handle nondeterministic and
ambiguous languages. An early paper by Griffiths and Petrick [18] describes
various parsing algorithms and their simulation by automata. There the amount
of nondeterminism is roughly specified by a selectivity matrix which guides the
parser in avoiding states leading to backtracking. A similar situation occurs in
the Prolog parsers described here. In compilers, interest is commonly restricted 
to deterministic languages. Backtracking may be prevented by a judicious use of
cuts(!) and/or by introducing assertions in the database that guide the parser in
avoiding dead-ends. 

A word about notation is in order. The grammar conventions are those in [l].
Edinburgh Prolog uses capital letters as variables, and therefore capitals cannot
be used to represent nonterminals unless they are quoted. In this paper, the
terms t( ) and n( ) denote, respectively, terminals and nonterminals. Quoting
may be necessary for specifying certain terminals (e.g., parentheses). For example,
the right-hand side (rhs) of the rule
\[F \rightarrow (E)\]
is described by the list 
\[[WV), de), W)‘)l. \]

Whenever stacks are used, they are also represented by lists whose leftmost
element is the top of the stack. 

This section does not pretend to make an exhaustive treatment of parsers.
We describe bottom-up and top-down parsers for both nondeterministic and
deterministic languages. A nondeterministic shift-reduce and a deterministic
weak-precedence parser are the bottom-up representatives. Their top-down counterparts
are, respectively, a predictive and an LL(1) parser. A recursive descent
version of the latter is also considered. Besides those described herein, we have
programmed and tested Earley’s algorithm [13] and a parser generator that
produces the necessary tables for parsing SLR( 1) grammars [ 11]. 

\secdown

\secrel{Bottom-Up}

A very simple (albeit inefficient) shift-reduce parser can be readily programmed
in Prolog. Its action consists of attempting to reduce whenever possible; otherwise
the window is shifted on to a stack and repeated reductions (followed by shifts)
take place until the main nonterminal appears by itself in the top of the stack.
Note that a reduction may be immediately followed by other reductions. A
reduction corresponds to the recognition of a grammar rule; for instance, the
reduction for the rule E + E + T occurs when E + T lies on the top of the stack.
It is then replaced by an E. This action is expressed by the unit clause
\[redme(Idt), H+), n(e) I Xl, Me) I Xl). \]

Let us consider the classical grammar describing arithmetic expressions: 
\[G1: E+E+T\]
\[E-T\]
\[T-+ T*F\]
\[T+F\]
\[F + (E)\]
\[F + (letter)\]
The appropriate sequence of reduce clauses follows immediately from the above
rules. To decrease the amount of backtracking it is convenient to order these
clauses so that rules with longer right-hand sides are tried before those with 
shorter rhs. We are now ready to present the parser. It has two parameters:
(1) a list representing the string being parsed, and (2) the list representing the
current stack. 
\begin{verbatim}
% try-reduce %
sr-parse(Input, Stack) :- reduce(Stack, NewStack),
% try-shift %
sr-parse(Input, NewStack).
sr-parse([ Window I Rest], Stack) :- sr-parse(Rest, [Window 1 Stack]). 
\end{verbatim} 
Assume that a marker (\$) is to be placed at the end of each input string. The
following acceptance clause accepts a string only when the marker is in the
window and the stack contains just an E.
\begin{verbatim}
% acceptance %
sr-paWL§l, Me)lh 
\end{verbatim}
Consider the input string a*b. We assume that a scanner is available to translate
it into the suitable list, understandable by the parser. Then the query
\begin{verbatim}
?- sr-PamWa), t(*), t(b), $1, [ I).
\end{verbatim}
will succeed. 

The above parser is very inefficient, since it relies heavily on backtracking to
eventually accept or refuse a string. Note that in parsing the string a*b, t(a) is
first shifted and successively reduced to an F, T, and (even) an E; the latter
being a faulty reduction. The parser is, however, capable of undoing these
reductions through backtracking. This inordinate amount of backtracking can
be controlled by a careful selection of the reductions and shifts that eliminate
possible blind-alleys. This is done in our next bottom-up parser, which is the
weak-precedence type [ 1,19].

The basic strategy is to consult a table made of unit clauses like
\[try-redue(Top-of-stack, Window). and try_shift(Top-of-stack, Window).\]
which command a reduction or a shift, depending on the elements lying on the
window and on the top of the stack. The problem of automatically generating
the above clauses from the grammar rules is addressed in Section 5. The weakprecedence
relations for the grammar Gi are represented by the clauses 
\[try_reduce(n(t), \$).\]
\[try_reduce(n(f), \$9.\]
\[. . .\]
\[try-reduce(t(‘)‘), t(+)).\]
\[\&-reduce(t(‘)‘), t(‘)‘)).\]
and
\[try-shift(t(+), t(‘(‘)).\]
\[tryshift(n(e), t(+)).\]
and so on.    

We now transform the previous sr-parser into a wp-parser which takes
advantage of the additional information to avoid backtracking. Using 
Griffiths and Petrick’s terminology [ 18], these unit clauses render the
algorithms selective. 
\begin{verbatim}
% acceptance %
w-~~~~4$1, [de)l).
% try-reduce %
wp-parse([ W 1 Input], [S 1 Stack]) :- try_reduce(S, W),
reduce([S 1 Stack], NewStack),
wp-purse([ W 1 Input], NewStack).
% try shift %
wp-parse([ WI Input], [S 1 Stack]) :- try-shift(S, W),
wp-purse(Zrzput, [W, S 1 Stack]). 
\end{verbatim}
Notice that if a grammar is truly a weak-precedence grammar (i.e., there are no
precedence conflicts and rules have distinct rhs), then backtracking will only
occur when try-reduce fails and try-shift has to be tried. Thus the query
\begin{verbatim}
?- wp-parse(Znput, [ I), print(accept).
\end{verbatim}
will print “accept”, and succeed if the Input string is in the language. If the string
is not in the language, the query will fail. The time complexity is proportional to
the length of Input. Error detection and recovery are discussed in Section 9.

A comment about the efficiency of this version of wp-parse is in order. Since
there will in general be a large number of try-reduce and try-shift rules, the
execution time of the wp-parser could be significantly reduced if a Prolog compiler
could branch directly to a clause having the appropriate constant as its first term
(for example, by constructing a hashing table at compile time). Recent and
planned Prolog optimizing compilers can indeed perform this branching [30].
The reader should also refer to [21] for a discussion of optimizations applicable
to deterministic Prolog programs, which render their efficiency closer to those of
conventional programs. 

Finally, note that it would be straightforward to extend this type of parser to
cover the syntactical analysis of bounded-context grammars, that is, those for
which a decision to reduce or shift is based on an inspection of m elements in
the top of the stack and a look-ahead of n elements in the input string. 

\secrel{Top-Down}

A Prolog implementation of predictive parsers [l] follows readily from the
programs described in the previous section. The grammar G2, below, generates
the same language as G1, but left-recursion has been replaced by right-recursion. 
\[Gz: E -+ TE’\]
\[E’ -+ + TE’\]
\[E’ 3 e\]
\[T +FT’\]
\[T’ --B * FT’\]
\[T’ + E\]
\[F *U-O\]
\[F + (letter)\]
The above rules are placed in the database using the unit clauses
\begin{verbatim}
rule(Non-terminal, Rhs).
\end{verbatim}
Examples are
\begin{verbatim}
ruMn(tprim), [t(*), n(f), nt@hne)l).
rule(n(tprime), [ I). 
\end{verbatim}

The parser predict(Input, Stack) has the same parameters as its predecessors,
namely: (1) the input string and (2) the current stack contents (initially n(e),
where E is the main nonterminal). The parser succeeds if the Input string is in
the language, and fails otherwise. 

The basic action of predict is to replace a nonterminal on the top of the parse
stack by the rhs of the rule defining that nonterminal. If a terminal element lies
on the top of the stack and if it matches the element W in the window, then
parsing proceeds by popping W and considering the next element of the input
string to be in the window. A string is accepted when the stack is empty and the
window contains the marker. In Prolog we have 
\begin{verbatim}
% acceptance.
predict(I$l, 1 I).
% try a possible rule.
predict(lnput, [n(N) 1 Stack]) :-
ruldn(N), Rh),
append(Rhs, Stack, NewStack),
predict&put, NewStack).
% match the terminals.
predict([t(W) 1 Input], [t(W) 1 Stack]) :- predict(lnput, Stack). 
\end{verbatim}
The above parser can handle nondeterministic or even ambiguous grammars, but
may become trapped in an infinite recursion loop if the grammar is left-recursive. 

To improve the efficiency when processing deterministic grammars, one could
again resort to placing additional information in the database. This is the case
for the next parser we consider, which is applicable to LL( 1) grammars, and does
not rely on backtracking. It will become apparent in Section 5 that it is straightforward
to generate tables for LL( 1) grammars [ 1]. These tables have as entries
the contents of the window t(W) and the nonterminal n(N) on the top of the
stack, and they specify the appropriate (unique) replacement by the rhs of the
rule defining N. Entries may be defined by unit clauses of the form 
\begin{verbatim}
entv(t(W), n(N), Rhs).
\end{verbatim}
for all pairs ( W, N) such that N J* W . . . . An LL(1) deterministic parser is
obtained by replacing the middle clause of predict by
\begin{verbatim}
predict[t( W) 1 Input], [n(N) 1 Stack]) :- entry(t(W), n(N), Rhs),
append(Rhs, Stack, NewStack),
predict([t(W) I Input], NewStack).   
\end{verbatim}

By properly selecting one among multiple entries, predict can deterministically
parse languages defined by ambiguous grammars, as is the case of the if then
else construct considered in [l, p. 191]. Moreover, the parser does not rely on
backtracking to accept a string. The complexity of the LL(1) parser is therefore
O(n) where n is the length of the input string.

 

\secup

\secup