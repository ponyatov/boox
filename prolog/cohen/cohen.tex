\secrel{Parsing and Compiling Using Prolog}\label{cohen}\secdown
\href{https://drive.google.com/file/d/0B0u4WeMjO894eHpLcTE2bWU0SjQ/view?usp=sharing}{pdf}
\cp{\url{https://pdfs.semanticscholar.org/dd8d/c0deb336d90912a21ba8ec6f6c6fef4b4024.pdf}}

\copyright\ JACQUES COHEN and TIMOTHY J. HICKEY\\
Brandeis University
\bigskip

\subsecly{Abstract}

This paper presents the material needed for exposing the reader to the
advantages of using Prolog as a language for describing succinctly most of the
algorithms needed in prototyping and implementing compilers or producing tools
that facilitate this task. The available published material on the subject
describes one particular approach in implementing compilers using Prolog. It
consists of coupling actions to recursive descent parsers to produce
syntax-trees which are subsequently utilized in guiding the generation of
assembly language code. Although this remains a worthwhile approach, there is a
host of possibilities for Prolog usage in compiler construction. The primary aim
of this paper is to demonstrate the use of Prolog in parsing and compiling. A
second, but equally important, goal of this paper is to show that Prolog is a
labor-saving tool in prototyping and implementing many nonnumerical algorithms
which arise in compiling, and whose description using Prolog is not available in
the literature. The paper discusses the use of unification and nondeterminism in
compiler writing as well as means to bypass these (costly) features when they
are deemed unnecessary. Topics covered include bottom-up and top-down parsers,
syntax-directed translation, grammar properties, parser generation, code
generation, and optimixations. Newly proposed features that are useful in
compiler construction are also discussed. A knowledge of Prolog is assumed.

Categories and Subject Descriptors: D.l.O [Programming Techniques]: General;
D.2.m [Software Engineering]: Miscellaneous--rapid prototyping; D.3.4
[Programming Languages]: Processors; F.4.1. [Mathematical Logic and Formal
Languages]: Mathematical Logic--logic programming 1.2.3 [Artificial
Intelligence]: Deduction and Theorem Proving--logic programming

General Terms: Algorithms, Languages, Theory, Verification

Additional Key Words and Phrases: Code generation, grammar properties,
optimization, parsing

\bigskip
This work was supported by the NSF under grant DCR 8590881.

Authors’ address: Computer Science Department, Ford Hall, Brandeis University, Waltham, MA
02254.

Permission to copy without fee all or part of this material is granted provided that the copies are not
made or distributed for direct commercial advantage, the ACM copyright notice and the title of the
publication and its date appear, and notice is given that copying is by permission of the Association
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific
permission.

0 1987 ACM 0164-0925/87/0400-0125 \$00.75

ACM Transactions on Programming Languages and Systems, Vol. 9, No. 2, April 1967, Pages 125-163. 

\secrel{INTRODUCTION}

The seminal paper by Alain Colmerauer on Metamorphosis Grammars first
appeared in 1975 [9]. That paper spawned most of the developments in compiler
writing using Prolog, a great many of them due to David H. D. Warren. Warren’s
thesis [30], the paper summarizing it [31], and the related work on Definite
Clause Grammars [25] are practically the sole sources of reference on the
subject.\note{A recent book edited by Campbell [3] mostly covers the
implementation of Prolog itself. }

The available published material on the subject describes one particular approach
in implementing compilers using Prolog. It consists of coupling actions 
to recursive descent parsers to produce syntax-trees which are subsequently
utilized in guiding the generation of assembly language code. Although this
remains a worthwhile approach, there is a host of possibilities for Prolog usage
in compiler construction. The primary aim of this paper is to present the material
needed for exposing the reader to the advantages of using Prolog in parsing and
compiling. A second, but equally important, goal of this paper is to show that
Prolog is a labor-saving tool in prototyping and implementing many nonnumerical
algorithms which arise in compiling, and whose description using
Prolog is not available in the literature. Finally, a third goal is to present new
approaches to compiler design which use proposed extensions to Prolog. 

This paper is directed to compiler designers moderately familiar with Prolog,
who wish to explore the advantages and present drawbacks of using this language
for implementing language processors. The advantages of Prolog stem from two
important features of the language. 

(1) The use of unification as a general pattern-matching operation allowing
procedure parameters (logical variables) to be both input and output or to
remain unbound. Unification replaces the conditionals and assignments
which exist in most languages. 

(2) The ability to cope with nondeterministic situations, and therefore allow the
determination of multiple solutions to a given problem. 

From a subjective point of view, the main advantage of Prolog is that the
language has its foundations in logic, and it therefore encourages the user to
describe problems in a logical manner which facilitates the checking for correctness,
enhances program readability, and reduces the debugging effort. It will be
seen that unification and nondeterminism play an important role in compiler
design; however, using their full generality is often costly and unnecessary. These
issues are discussed throughout the paper whenever they become relevant.
Remarks are made in the last section about the efficiency of Prolog-written
compilers and the means to improve their performance. 

The Prolog proficiency assumed in this paper can be acquired by reading the
first few chapters of either Kowalski’s [20] or Clocksin and Mellish’s [6] books.
In particular, the reader should be at ease with elementary list processing and
with the predicate append. The concrete syntax used in this paper is that of
Edinburgh Prolog [6]. It is also assumed that the reader is familiar with compiler
design topics such as parsing, lexical analysis, code generation, optimizations,
and so on. These topics are covered in standard texts [l, 17,29]. 

\secrel{PARSING}

In this section we present parsers belonging to two main classes of parsing
algorithms, namely, bottom-up and top-down. Due to the backtracking capabilities
of Prolog, these parsers can in general handle nondeterministic and
ambiguous languages. An early paper by Griffiths and Petrick [18] describes
various parsing algorithms and their simulation by automata. There the amount
of nondeterminism is roughly specified by a selectivity matrix which guides the
parser in avoiding states leading to backtracking. A similar situation occurs in
the Prolog parsers described here. In compilers, interest is commonly restricted 
to deterministic languages. Backtracking may be prevented by a judicious use of
cuts(!) and/or by introducing assertions in the database that guide the parser in
avoiding dead-ends. 

A word about notation is in order. The grammar conventions are those in [l].
Edinburgh Prolog uses capital letters as variables, and therefore capitals cannot
be used to represent nonterminals unless they are quoted. In this paper, the
terms t( ) and n( ) denote, respectively, terminals and nonterminals. Quoting
may be necessary for specifying certain terminals (e.g., parentheses). For example,
the right-hand side (rhs) of the rule
\[F \rightarrow (E)\]
is described by the list 
\[[WV), de), W)‘)l. \]

Whenever stacks are used, they are also represented by lists whose leftmost
element is the top of the stack. 

This section does not pretend to make an exhaustive treatment of parsers.
We describe bottom-up and top-down parsers for both nondeterministic and
deterministic languages. A nondeterministic shift-reduce and a deterministic
weak-precedence parser are the bottom-up representatives. Their top-down counterparts
are, respectively, a predictive and an LL(1) parser. A recursive descent
version of the latter is also considered. Besides those described herein, we have
programmed and tested Earley’s algorithm [13] and a parser generator that
produces the necessary tables for parsing SLR( 1) grammars [ 11]. 

\secdown

\secrel{Bottom-Up}

A very simple (albeit inefficient) shift-reduce parser can be readily programmed
in Prolog. Its action consists of attempting to reduce whenever possible; otherwise
the window is shifted on to a stack and repeated reductions (followed by shifts)
take place until the main nonterminal appears by itself in the top of the stack.
Note that a reduction may be immediately followed by other reductions. A
reduction corresponds to the recognition of a grammar rule; for instance, the
reduction for the rule E + E + T occurs when E + T lies on the top of the stack.
It is then replaced by an E. This action is expressed by the unit clause
\[redme(Idt), H+), n(e) I Xl, Me) I Xl). \]

Let us consider the classical grammar describing arithmetic expressions: 
\[G1: E+E+T\]
\[E-T\]
\[T-+ T*F\]
\[T+F\]
\[F + (E)\]
\[F + (letter)\]
The appropriate sequence of reduce clauses follows immediately from the above
rules. To decrease the amount of backtracking it is convenient to order these
clauses so that rules with longer right-hand sides are tried before those with 
shorter rhs. We are now ready to present the parser. It has two parameters:
(1) a list representing the string being parsed, and (2) the list representing the
current stack. 
\begin{verbatim}
% try-reduce %
sr-parse(Input, Stack) :- reduce(Stack, NewStack),
% try-shift %
sr-parse(Input, NewStack).
sr-parse([ Window I Rest], Stack) :- sr-parse(Rest, [Window 1 Stack]). 
\end{verbatim} 
Assume that a marker (\$) is to be placed at the end of each input string. The
following acceptance clause accepts a string only when the marker is in the
window and the stack contains just an E.
\begin{verbatim}
% acceptance %
sr-paWL§l, Me)lh 
\end{verbatim}
Consider the input string a*b. We assume that a scanner is available to translate
it into the suitable list, understandable by the parser. Then the query
\begin{verbatim}
?- sr-PamWa), t(*), t(b), $1, [ I).
\end{verbatim}
will succeed. 

The above parser is very inefficient, since it relies heavily on backtracking to
eventually accept or refuse a string. Note that in parsing the string a*b, t(a) is
first shifted and successively reduced to an F, T, and (even) an E; the latter
being a faulty reduction. The parser is, however, capable of undoing these
reductions through backtracking. This inordinate amount of backtracking can
be controlled by a careful selection of the reductions and shifts that eliminate
possible blind-alleys. This is done in our next bottom-up parser, which is the
weak-precedence type [ 1,19].

The basic strategy is to consult a table made of unit clauses like
\[try-redue(Top-of-stack, Window). and try_shift(Top-of-stack, Window).\]
which command a reduction or a shift, depending on the elements lying on the
window and on the top of the stack. The problem of automatically generating
the above clauses from the grammar rules is addressed in Section 5. The weakprecedence
relations for the grammar Gi are represented by the clauses 
\[try_reduce(n(t), \$).\]
\[try_reduce(n(f), \$9.\]
\[. . .\]
\[try-reduce(t(‘)‘), t(+)).\]
\[\&-reduce(t(‘)‘), t(‘)‘)).\]
and
\[try-shift(t(+), t(‘(‘)).\]
\[tryshift(n(e), t(+)).\]
and so on.    

We now transform the previous sr-parser into a wp-parser which takes
advantage of the additional information to avoid backtracking. Using 
Griffiths and Petrick’s terminology [ 18], these unit clauses render the
algorithms selective. 
\begin{verbatim}
% acceptance %
w-~~~~4$1, [de)l).
% try-reduce %
wp-parse([ W 1 Input], [S 1 Stack]) :- try_reduce(S, W),
reduce([S 1 Stack], NewStack),
wp-purse([ W 1 Input], NewStack).
% try shift %
wp-parse([ WI Input], [S 1 Stack]) :- try-shift(S, W),
wp-purse(Zrzput, [W, S 1 Stack]). 
\end{verbatim}
Notice that if a grammar is truly a weak-precedence grammar (i.e., there are no
precedence conflicts and rules have distinct rhs), then backtracking will only
occur when try-reduce fails and try-shift has to be tried. Thus the query
\begin{verbatim}
?- wp-parse(Znput, [ I), print(accept).
\end{verbatim}
will print “accept”, and succeed if the Input string is in the language. If the string
is not in the language, the query will fail. The time complexity is proportional to
the length of Input. Error detection and recovery are discussed in Section 9.

A comment about the efficiency of this version of wp-parse is in order. Since
there will in general be a large number of try-reduce and try-shift rules, the
execution time of the wp-parser could be significantly reduced if a Prolog compiler
could branch directly to a clause having the appropriate constant as its first term
(for example, by constructing a hashing table at compile time). Recent and
planned Prolog optimizing compilers can indeed perform this branching [30].
The reader should also refer to [21] for a discussion of optimizations applicable
to deterministic Prolog programs, which render their efficiency closer to those of
conventional programs. 

Finally, note that it would be straightforward to extend this type of parser to
cover the syntactical analysis of bounded-context grammars, that is, those for
which a decision to reduce or shift is based on an inspection of m elements in
the top of the stack and a look-ahead of n elements in the input string. 

\secrel{Top-Down}

A Prolog implementation of predictive parsers [l] follows readily from the
programs described in the previous section. The grammar G2, below, generates
the same language as G1, but left-recursion has been replaced by right-recursion. 
\[Gz: E -+ TE’\]
\[E’ -+ + TE’\]
\[E’ 3 e\]
\[T +FT’\]
\[T’ --B * FT’\]
\[T’ + E\]
\[F *U-O\]
\[F + (letter)\]
The above rules are placed in the database using the unit clauses
\begin{verbatim}
rule(Non-terminal, Rhs).
\end{verbatim}
Examples are
\begin{verbatim}
ruMn(tprim), [t(*), n(f), nt@hne)l).
rule(n(tprime), [ I). 
\end{verbatim}

The parser predict(Input, Stack) has the same parameters as its predecessors,
namely: (1) the input string and (2) the current stack contents (initially n(e),
where E is the main nonterminal). The parser succeeds if the Input string is in
the language, and fails otherwise. 

The basic action of predict is to replace a nonterminal on the top of the parse
stack by the rhs of the rule defining that nonterminal. If a terminal element lies
on the top of the stack and if it matches the element W in the window, then
parsing proceeds by popping W and considering the next element of the input
string to be in the window. A string is accepted when the stack is empty and the
window contains the marker. In Prolog we have 
\begin{verbatim}
% acceptance.
predict(I$l, 1 I).
% try a possible rule.
predict(lnput, [n(N) 1 Stack]) :-
ruldn(N), Rh),
append(Rhs, Stack, NewStack),
predict&put, NewStack).
% match the terminals.
predict([t(W) 1 Input], [t(W) 1 Stack]) :- predict(lnput, Stack). 
\end{verbatim}
The above parser can handle nondeterministic or even ambiguous grammars, but
may become trapped in an infinite recursion loop if the grammar is left-recursive. 

To improve the efficiency when processing deterministic grammars, one could
again resort to placing additional information in the database. This is the case
for the next parser we consider, which is applicable to LL( 1) grammars, and does
not rely on backtracking. It will become apparent in Section 5 that it is straightforward
to generate tables for LL( 1) grammars [ 1]. These tables have as entries
the contents of the window t(W) and the nonterminal n(N) on the top of the
stack, and they specify the appropriate (unique) replacement by the rhs of the
rule defining N. Entries may be defined by unit clauses of the form 
\begin{verbatim}
entv(t(W), n(N), Rhs).
\end{verbatim}
for all pairs ( W, N) such that N J* W . . . . An LL(1) deterministic parser is
obtained by replacing the middle clause of predict by
\begin{verbatim}
predict[t( W) 1 Input], [n(N) 1 Stack]) :- entry(t(W), n(N), Rhs),
append(Rhs, Stack, NewStack),
predict([t(W) I Input], NewStack).   
\end{verbatim}

By properly selecting one among multiple entries, predict can deterministically
parse languages defined by ambiguous grammars, as is the case of the if then
else construct considered in [l, p. 191]. Moreover, the parser does not rely on
backtracking to accept a string. The complexity of the LL(1) parser is therefore
O(n) where n is the length of the input string.

\secrel{Recursive Descent }

All of the previously described parsers contain a general nucleus which drives
the parsing, the grammar rules being specified by unit clauses in the database.
Parser efficiency can be increased by establishing a direct mapping between
grammar rules and Prolog clauses. This is accomplished as in recursive descent
parsing: each procedure directly corresponds to a given grammar rule. As usual,
left-recursion is not allowed and has to be replaced by right-recursion to avoid
endless loops. 

There are three manners in which these parsers can be implemented in Prolog,
depending on the form of the input string. The first and the least efficient of
these is the one that uses the predicate append. The second uses links to define
the input string that appears as unit clauses in the database. Finally, the third,
which uses difference lists, is the most efficient, as will be seen by estimates of
the various complexities. The implementation of these versions is illustrated
using the grammar G3, generating a’kb”, n I 0. The notation t(T) and n(N) will
no longer be needed to differentiate between terminals and nonterminals, since
the nonterminals will be transformed into Prolog procedures which manipulate
terminal strings. 
\[G3: S+aSb\]
\[S4C\]
Every grammar rule is transformed into a clause whose argument is the list of
terminals derived from the defined nonterminal. Terminals are similarly handled
using unit clauses. We have 
\begin{verbatim}
s(ASB) :- uppend(A, SB, ASB),
~PP~W, B, SB),
a(A),
SW,
MB).
s(C) :- c(C).
ml).
WI).
4cl).   
\end{verbatim}
The appends are used to partition the list ASB as the concatenation of three
sublists A, S, B. Although the only partition for which the parser will succeed is
\[A = a, S = an-lcbnml, B = b,\]
this program will generate at least 2n incorrect partitions. Hence the number of
calls needed to append is at least n2. Note that the appends should precede the
calls of a(A), s(S), b(B). Otherwise, an infinite loop would occur. The above
program can be optimized by symbolic execution: the terms a(A), b(B), and c(C)
can be directly replaced by their unit clause counterparts, yielding 
\begin{verbatim}
s(ASB) :- uppend([a], SB, ASB),
wwMS, PI, SB),
SW.
ml).  
\end{verbatim}

The second approach for programming recursive descent parsers in Prolog is
the use of links. An input string such as [a, a, c, b, b] is represented by the unit
clauses link(i, t, i + l), stating that there is a terminal t located between positions
i and i + 1. In our case the input string aacbb becomes
hk(1, a, 2).
\begin{verbatim}
link(2, a, 3).
link(3, c, 4).
link(4, b, 5).
link(5, b, 6). 
\end{verbatim}
A clause recognizing a nonterminal will now have two parameters denoting the
leftmost and rightmost positions in the input string that will parse into the given
nonterminal. In our particular example we have
\begin{verbatim}
s(X1, X4) :- link(X1, a, X2),
s(X2, X3),
link(X3, b, X4).
s(X1, X2) :- link(X1, c, X2). 
\end{verbatim}
The as will be consumed by the n successive calls of the first two literals. Then,
only the second clause is applicable and the c is consumed. Finally, the unbound
variables X3, X4 are successively bound to the points separating the remaining
bs. The algorithm’s complexity is therefore linear. 

An efficient implementation of recursive descent parsers in Prolog makes use
of difference lists. If a nonterminal A generates a terminal string a! (i.e., A ==a*
a), that string can be represented by the difference of two lists U and V; V is a
sublist of U which has the same tail as U. For example, if U is [a, c, b, b, b] and
V is [b, b] the difference U - V defines the list [a, c, b], which for G3 parses into
an S. Warren [31] points out that the use of difference lists corresponds to having
the general link-like clause:
\begin{verbatim}
link([H 1 2’1, H, T) 
\end{verbatim}
which can be read as “the string position labelled by the list with head H and
tail T is connected by a symbol H to the string position labelled T.” A parser for
G3 using difference lists can be written as follows:
\begin{verbatim}
s(U, V) :- a(U, Vl), s(V1, V2), b(V2, V).
s( w, 2) :- c( w, 2). 
\end{verbatim}
For the terminals a, b, and c we have
\begin{verbatim}
am I VII, vu.
b([b I U21, U2).
c(k I u31, U3). 
\end{verbatim}
Symbolic execution allows us to find the values of U and Vl in the first clause:
\begin{verbatim}
U=[alUl], Vl=Ul 
\end{verbatim}
Similarly,
\begin{verbatim}
V2=[bIU2], V= U2
W=[cIu@], z=u3 
\end{verbatim}
Substituting the values of the above variables, we obtain the optimized program
\begin{verbatim}
s([a 1 Ul], U2) :- s(U1, [b I U2]).
4kl u31, U3). 
\end{verbatim}
(The above program could also have been derived using symbolic execution
by considering the first version of the parser with append and noticing that
if X - Y and Y - Z are difference lists, then append(X - Y, Y - 2, X - 2) is
a fact.) 

Let us follow the execution of the call
\[s(b, 0, c, b, bl, [ I).\]
Notice that Ul becomes [a, c, b, b] and U2 is [ ]. The next calls of S are
\[~([a, c, h bl, PII\]
\[s(k, h bl, P, bl)\] 
This last call matches only the second clause thus indicating a valid string. An
informal English description of the acceptance is as follows: successively remove
each a in the head of the first of the difference lists and add a b to the second
one. A string is accepted when no more as can be removed, the head of the first
list is a c, and the two lists contain the same number of bs. Therefore, for this
particular grammar, G3, the parsing is done in linear time with no backtracking.
The reader might have already surmised that the use of difference lists and of
symbolic executions illustrated in this example could be carried out automatically
from the given grammar rules. Clocksin and Mellish ([6, 1st ed., p. 237-2381]
present a short Prolog program that does the translation. 

\secup

\secrel{SYNTAX-DIRECTED TRANSLATION }

This type of translation consists of triggering semantic actions specified by the
programmer, once selected syntactic constructs are found by a parser. In the case
of the bottom-up parsers described in Section 2.1, it suffices to add a third
parameter to the reduce clauses specifying the rule number and to modify the
parser so that a semantic action (specified by the rule number) will take place
just after the reduction. For example, in order to translate arithmetic expressions
into postfix Polish notation, the corresponding reduce for the first rule of G1
becomes
\begin{verbatim}
reduce(W), t(+), n(e) I Xl, [n(e) I Xl, 0
\end{verbatim}
The modified parser contains two additional parameters: (1) a stack, Sem, which
will be manipulated by the action procedure and (2) a parameter, Result, which
will be bound to the final result of the semantic actions:
\begin{verbatim}
% accept and bind Result to the semantic parameter.
wp-trunslate([$], [n(e)], Result, Result).
% try to perform a reduction and a semantic action.
wp-transkzte([ W 1 Input], [S I Stack], Sem, Result) :-
&y-reduce@, W),
reduce([S I Stack], NewStack, RuleNumber),
action(RuleNumber, [S I Stuck], Sem, NewSem),
wp-translute([ W I Input], NewStack, NewSem, Result). 
\end{verbatim}
\begin{verbatim}
% try a shift.
wp-transZate([ W 1 Input], [S 1 Stack], Sem, Result) :-
try_shift(S, W),
wp-translute(Znput, [W, S 1 Stack], Sem, Result).  
\end{verbatim}
The parser can then he equipped with actions by adding rules which specify how
the temporary semantic parameter is to be modified for each rule. The following
action procedure constructs parse trees for the arithmetic expressions defined by
grammar G1:
\begin{verbatim}
syntax-tree(Znput, Tree) :- wp-trandute(Znput, [ 1, [ 1, Tree).
action(1, Stack, [Xl, X2 1 T], [plus(X2, Xl) IT]).
action(3, Stack, [Xl, X2 I 2’1, [times(X2, Xl) I 2’1).
action(6, [t(Z,etter) I Stack], Temp, [Letter I Temp]).
action(X, Stack, Temp, Temp) :- X # 1, X # 3, X # 6. 
\end{verbatim}
The body of the last clause guarantees that no spurious actions are performed
should backtracking ever occur. Notice that the action procedure must have
access to the parsing stack (as is the case for rule 6) so that specific terminals
may be incorporated into the actions. A similar strategy is applicable in adding
actions to predictive parsers. 

All of the above descriptions of semantic actions utilize inherited attributes
and are admittedly standard. The main purpose of presenting them here is to
point out how succinct the descriptions become when Prolog is used. The truly
novel way of performing syntax-directed translation is that pioneered by
Colmerauer and widely utilized by Warren. That approach does not strictly
separate syntax from semantics as was done in this section. They have added
new parameters to the recursive descent parser described in Section 2.3, so that
the translation takes full advantage of the unification and goal-seeking features
of Prolog. Colmerauer’s approach is the subject of the next section. 

\secrel{M-GRAMMARS AND DCGs }\label{cohen3}

A metamorphosis (or M-) grammar is a formalism which combines a Chomskytype
language definition with logic programming capabilities for manipulating
the semantic attributes needed to perform syntax-directed translations.
Colmerauer [9] maps general type-0 Chomsky rules into general logicprogramming
clauses, (i.e., those that may contain more than one predicate in
the left-hand side). A very useful subset of M-Grammars are Definite Clause
Grammars (DCGs), which are based on Chomsky’s context-free grammars. The
reader has undoubtedly noticed the similarity between Prolog clauses and context-free
grammar rules: they both have one term in the lhs and several (or none,
i.e., t) in the rhs. Prolog restricts itself to those special clauses called Horn or
Definite clauses, thus explaining the acronym. It will be seen shortly that
although DCGs are based on context-free grammars they are able to parse
context-sensitive ones as well. (In fact, any recursively enumerable language can
be recognized using DCGs with parameters.) 

DCGs are translated directly into Prolog clauses which include a recursive
descent parser using difference lists. For example, the DCG rules for recognizing
strings in G3 are
\[s --+ [cl.\]
\[s --+ [a], s, [b].\]
The syntax of DCGs is close to that of Prolog clauses. The ‘:-’ is replaced
by ‘--+‘, and terminals appear within square brackets. Most Prolog interpreters
automatically translate the above into the clauses:
\[s([c I m, LO).\]
\[s([a 1 LO], Ll) :- s(L0, [b I Ll]).\]
which have already been explained in Section 2.3. DCG terms usually contain
one or more arguments which are directly copied into their Prolog counterparts,
which also contain the difference list parameters. Our first example of usage of
DCGs is to determine the value of n for a given input string a”\&” (generated by
grammar GB ).
\[s(0) --+ [cl.\]
\[s(succW) --+ [al, SW), PI.\]
The added argument specifies that the recognition of a c implies a value of
N = 0. Each time an s surrounded by an a and a b is recognized, the value of N
increases by one (succ indicates the successor). The above DCGs are automatically
translated into
\[SK4 [c I w, LO).\]
\[s(succ(N), [a 1 LO], Ll) :- s(N, LO, [b I Ll]).\]
The call s(X, [a, a, c, b, b], [ ]) yields X = succ(succ(0)). The backtracking
capabilities of Prolog allow the call s(succ(succ(O)), X, [ ]) which yields X =
[a, a, c, b, b]. 

By employing a technique similar to the one illustrated by the previous
example, we can construct a parser s to recognize the language a”b”c”. It uses
the auxiliary procedure sequ.ence(X, N) (defined below) which parses a list of Xs
and binds N to the number of Xs found.
\[sequence(X, 0) --+ [ 1.\]
\[sequence(X, succ(N)) --+ [Xl, sequeme(X, IV)\]
\[s(N) --+ sequence(a, N), sequence(b, N), seqwme(c, IV)\]

Let us now consider the use of DCGs for translating arithmetic expressions
into their syntax-trees. We start with the simplified right-recursive grammar
rules:
\[E-T +E\]
\[E-T\]
\[T-a\]
Initially one would be tempted to use the DCG
\[eWw(X, Y)) --+ W3, [+I, e(y).\]
\[e(X) --+ t(X).\]
\[t(a) --9 [a].\]
These rules, however, translate a + a + a into plus (a, pZus(u, a)) which is rightassociative,
and therefore semantically incorrect. Some cunning is needed to 
circumvent this difficulty. Let us first rewrite the grammar rules as
\[E+TR\]
\[R++TR\]
\[R-,C\]
\[T+a\]
Our goal is to generate plus(plus(a, a), a) for the input string a + a + a. The
following DCG will do the proper translation:
\[eqv\$?3) --9 term(Tl), restexpr(T1, E).\]
\[restexpr(T1, E) --a [+I, term(T2), restezpr(plus(T1, T2), E).\]
\[restexpr(\&, E) --+ [ 1.\]
\[term(a) -3 [a].\]
When the clause expr recognizes the first term Tl in the expression, it passes
this term to the second clause, restexpr. If there is another term T2 following
Tl, then the composite term pb(T1, T2) is constructed and recursively passed
to restexpr. The first parameter of restexpr is used to build a left-recursive parse
tree, which is finally transmitted back to erpr by the third clause. 

Unfortunately, the above “contortions” are needed if one insists on using a
recursive descent parser to contruct a left-associative syntax tree. This particular
Prolog technique has become a standard idiom among DCG writers. A way out
of this predicament is to implement DCGs using bottom-up parsers. (This has
been proposed in [28].) At present these capabilities are not generally available
in existing Prolog interpreters and compilers. 

It is straightforward to generalize the above translation by introducing multilevel
grammar rules such as
\[Ei + TiRi\]
\[Ri + OpiTiRi\]
\[\& + \&+I\]
\[Ri + c\]
with 1 5 i 5 n and En+, + letter ] (El), where i denotes the precedence of the
operator opi. The corresponding DCG contains i as a parameter, and could allow
for redefining the priorities of the operators, therefore rendering the language
extensible. This approach is used in the Edinburgh version of Prolog.      

A very useful feature of DCGs is that parts of Prolog programs may appear in
their right-hand sides. This is done by surrounding the desired Prolog predicates
within curly brackets. Our next example illustrates the use of this feature to
perform the translation of arithmetic expressions into postfix notation directly
by a DCG that does not construct syntax trees. Our first example of this technique
will output the postfix notation.
\[e --+ t, r.\]
\[r --+ [+I, t, write(+)), r.\]
\[r --+ [ 1.\]
\[t --+ [a], (write(a)).\]
This procedure produces the postfix expression using side effects, and this
technique can be a drawback. One solution to this problem is to use difference 
lists to simulate the write procedure. To each DCG clause corresponding to a
nonterminal N we add difference list parameters representing the list of symbols
that are output during the recognition of N. (These difference lists are in addition
to those used in syntactic analysis). We have
\[e(F1, F3) --a t(F1, F2), r(F2, F3).\]
\[r(F1, F4) --+ [+I, t(F1, F2), writefik(+, F2, F3)), r(F3, F4).\]
\[r(F, F) --+ [ 1.\]
\[t(F1, F2) --+ [a], (writefik(a, Fl, F2)).\]
The output is simulated by the procedure writefile(Symbo1, Pos, NewPos) defined
by the unit clause
\[writefile(X, [X 1 B], B).\]

The call ?- e(F, [ 1, [a, +, a, +, a], [ ]]) produces F = [a, a, +, a, +]. This
example shows that difference lists can be used both to select parts of a list and to
construct a list. It is not hard to write a program that automatically performs
the translation from a DCG using write to a DCG using writefile and additional
difference lists. In the remainder of the paper we use the procedure write, and
leave to the interested reader the task of adding difference lists to avoid side
effects. 

The BNF of full-fledged programming languages can be readily transcribed
into DCGs that translate source programs into syntax-trees which can then be
either interpreted or used to generate code. We have tested the DCG needed to
process the entire Pascal language by translating input programs into syntaxtrees.
The following program fragments illustrate this construction for parts of a
mini-language. A while statement is defined by the DCG: 
\begin{verbatim}
statement(while(Test, Do)) --+
[while], test(Test), [do], statement(Do).
\end{verbatim}
A test may be defined by
\begin{verbatim}
test(test(Op, El, E2)) --+ expr(&l),
comP(oP),
expr(E2).
camp(=) --+ [=I.
cow(( )I --+ [( )I.
etc . . .  
\end{verbatim}
The translation of statements into P-code-like instructions is also easily achieved.
For example the statement while T do S can be directly translated into the
sequence
\begin{verbatim}
L: code for test T
jif(i.e, jump if false) to Exit
s
jump to L
Exit:       
\end{verbatim}
If labels are represented by terms of the form label(L) and the instructions by
instr(jif, L) or instr(jump, L), the translation is performed by the DCG:
\begin{verbatim}
statement([label(L), Test, S, instr( jump, L), lubel(Exit)]) --+
[while], test(Test, Exit), [do], statement(S). 
\end{verbatim}
where
\begin{verbatim}
test([Rl, R2, Op, in.str(jif, Exit)], Exit) --+
expr(E1, Rl),
comP(oP),
expr(E2, R2). 
\end{verbatim}
This example illustrates the elegant use of Prolog’s logical variables and unification
in compiling. Each of the variables L and Exit occur twice in the generated
code. When instantiated, each pair will be bound to the same actual value. This
instantiation may occur at a later stage when the final program is assembled and
storage is allocated. Even when using special compiler-writing tools such as
YACC, the implementation of similar constructs requires lengthier programs
since one has to keep track of locations that have to be updated when final
addresses become known. Prolog’s ability to postpone bindings is therefore of
great value in compiling. 

The advantage of using logical variables and delayed binding is also
apparent in managing symbol tables. Consider the procedure Zookup(ldentifier,
Property, Dictionary), in which Dictionary is a list containing the pairs
(identifier-property); lookup’s behavior is similar to that of the procedure
member(E, L) which tests if an element E is present or not in the list L. However,
lookup adds the pair to the Dictionary if it has not been previously added. We
have
\begin{verbatim}
lookup(l, P, [[I, P] 1 T] :- !.
lookup(1, P, [[II, Pl] 1 T] :- lookup(I, P, T).
\end{verbatim}
If lookup is initially called with an uninstantiated variable, the first clause will
create the new pair [I, P] as well as a new uninstantiated variable T. The cut is
needed to prevent backtracking once the desired pair is found or is created. 
Consider now the sequence of calls to lookup:
\begin{verbatim}
lookup(a, Xl, D), lookup(b, X2, D), lookup(a, X3, D). 
\end{verbatim}

The net effect of the above calls is to store the two pairs [a, Xl] and [b, X2] in
D and to bind X3 to Xl. Later on, when Xl and X2 are instantiated, X3 will
automatically be bound to the value of Xl. 

A similar approach is used in [31] to implement binary tables. In that paper,
table lookup is done in the code-generation phase after constructing the syntax
trees (see Section 7). If one wished to perform that operation while parsing, the
DCG rule defining a factor could be
\begin{verbatim}
WU, PI, D) --+ Went(Z (bokup(V, PI, D)J. 
\end{verbatim}
where ident(1) is constructed in a previous scanning pass and the property P is
determined while processing declarations. In this case lookup should be modified
to handle semantic errors such as undeclared identifiers. 


 
\secup